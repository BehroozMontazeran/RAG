{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSearch-index_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opensearch-py\n",
    "# pip install opensearch-py-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "# import opensearch_py_ml as oml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "\n",
    "host = 'localhost'\n",
    "port = 9200\n",
    "auth = ('admin', 'admin') # For testing only. Don't store credentials in code.\n",
    "ca_certs_path = certifi.where() # Provide a CA bundle if you use intermediate CAs with your root CA.\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    "    ca_certs = ca_certs_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'pubmed-articles'#'pubmed-articles-combined'\n",
    "index_body = {\n",
    "  'settings': {\n",
    "    'index': {\n",
    "      'number_of_shards': 4\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Check if the index already exists\n",
    "if not client.indices.exists(index=index_name):\n",
    "    # If the index does not exist, create it\n",
    "    response = client.indices.create(index=index_name, body=index_body)\n",
    "    print(\"Index created successfully:\", index_name)\n",
    "else:\n",
    "    print(\"Index\", index_name, \"already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data to the OpenSearch index\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "# Function to remove NaN values recursively\n",
    "def remove_nan(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: remove_nan(v) for k, v in obj.items() if v is not None and not isinstance(v, float) and not (isinstance(v, str) and v.lower() == 'nan')}\n",
    "    elif isinstance(obj, list):\n",
    "        return [remove_nan(elem) for elem in obj if elem is not None and not isinstance(elem, float) and not (isinstance(elem, str) and elem.lower() == 'nan')]\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "file_name = 'additional_data' #articles\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(f'{file_name}.csv')\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries (JSON format)\n",
    "docs = df.to_dict(orient='records')\n",
    "\n",
    "# Remove NaN values from the data\n",
    "cleaned_data = remove_nan(docs)\n",
    "\n",
    "# Save the cleaned data back to a JSON file\n",
    "with open(f'{file_name}.json', 'w') as f:\n",
    "    json.dump(cleaned_data, f, indent=4)\n",
    "\n",
    "# Read the JSON file\n",
    "with open(f'{file_name}.json', 'r') as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "# Prepare the actions for bulk indexing\n",
    "actions = []\n",
    "for idx, doc in enumerate(docs):\n",
    "    action = {\n",
    "        'index': {\n",
    "            '_index': index_name,  # Specify the index name\n",
    "            '_id': idx#doc['PMID']  # Use a unique identifier for each document\n",
    "        }\n",
    "    }\n",
    "    actions.append(action)\n",
    "    actions.append(doc)  # Add the document itself as the next action\n",
    "\n",
    "# Perform the bulk operation\n",
    "batch_size = 1000  \n",
    "\n",
    "for i in tqdm(range(0, len(actions), batch_size)):\n",
    "    batch_actions = actions[i:i+batch_size]\n",
    "    client.bulk(batch_actions) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  -Dataset: additional_data.csv\n",
    "  -Columns: PMID, CD, source\n",
    "  -Index: pubmed-articles-combined\n",
    "  *****************************\n",
    "  -Dataset: articles.csv\n",
    "  -Columns: PMID,TI,AB,PB,FAU,FED,DP,OTO,OT,OWN,DCOM,LR,JT,MH,ISBN\n",
    "  -Index: pubmed-articles\n",
    "\"\"\"\n",
    "q = 'which article is published in 2015?'#'Peripheral plasma'\n",
    "query = {\n",
    "  'size': 5,  # Return only 5 documents\n",
    "  'query': {\n",
    "    'multi_match': {\n",
    "      'query': q,\n",
    "      'fields': ['CD']\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body=query,\n",
    "    index=index_name\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.delete(\n",
    "    index = 'python-test-index',\n",
    "    id = '1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.indices.delete(\n",
    "    index = 'pubmed-articles'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSerach- Match_serach- Neural_search- Hybrid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://opensearch.org/docs/latest/search-plugins/neural-search-tutorial/#prerequisites\"\n",
    "from opensearchpy import OpenSearch\n",
    "# import opensearch_py_ml as oml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "\n",
    "host = 'localhost'\n",
    "port = 9200\n",
    "auth = ('admin', 'admin') # For testing only. Don't store credentials in code.\n",
    "ca_certs_path = certifi.where() # Provide a CA bundle if you use intermediate CAs with your root CA.\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    "    ca_certs = ca_certs_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prerequisites\n",
    " update ML-related cluster settings:\n",
    " using an OpenSearch-provided machine learning (ML) model and a cluster with no dedicated ML nodes.\n",
    "   To ensure that this basic local setup works.\n",
    "  \"\"\"\n",
    "# Define the request body\n",
    "cluster_settings = {\n",
    "  \"persistent\": {\n",
    "    \"plugins\": {\n",
    "      \"ml_commons\": {\n",
    "        \"only_run_on_ml_node\": \"false\",\n",
    "        \"model_access_control_enabled\": \"true\",\n",
    "        \"native_memory_threshold\": \"99\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Send the request to set cluster settings\n",
    "response = client.cluster.put_settings(body=cluster_settings)\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "register a model group with the access mode set to public\n",
    "    \"\"\"\n",
    "\n",
    "# Define the request body\n",
    "body_data = {\n",
    "    \"name\": \"NLP_model_group\",\n",
    "    \"description\": \"A model group for NLP models\",\n",
    "    \"access_mode\": \"public\"\n",
    "}\n",
    "\n",
    "# Send the request to register a model group\n",
    "response = client.transport.perform_request(\n",
    "    method=\"POST\", url=\"/_plugins/_ml/model_groups/_register\", body=body_data\n",
    ")\n",
    "\n",
    "response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "register the model to the model group, provide the model group ID in the register request\n",
    "    \"\"\"\n",
    "\n",
    "# Define the request body\n",
    "body_data = {\n",
    "  \"name\": \"huggingface/sentence-transformers/msmarco-distilbert-base-tas-b\",\n",
    "  \"version\": \"1.0.1\",\n",
    "  \"model_group_id\": \"C83zwo0B8eW0xsky_4nW\",#f\"{response['model_group_id']}\",# \"C83zwo0B8eW0xsky_4nW\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\"\n",
    "}\n",
    "# Send the request to register a model group\n",
    "response = client.transport.perform_request(\n",
    "    method=\"POST\", url=\"/_plugins/_ml/models/_register\", body=body_data\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Check if the task is completed\"\n",
    "\n",
    "# Define the URL for the GET request\n",
    "url = f\"/_plugins/_ml/tasks/X7L6zI0BvZ3wLFxzveia\"#{response['task_id']}\" #G835wo0B8eW0xskyPomh\"\n",
    "\n",
    "# Send the request to register a model group\n",
    "response = client.transport.perform_request(method=\"GET\", url=url)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"When the model is registered, it is saved in the model index.\n",
    " Next, deploy the model. Deploying a model creates a model instance \n",
    " and caches the model in memory.\"\"\"\n",
    "\n",
    "# Define the URL for the GET request {response['model_id']}\n",
    "url = f\"/_plugins/_ml/models/ZnT6zI0B5EQu2yufwsQj/_deploy\" #nfH5wo0BdZ4UfKGGQ7gm\n",
    "\n",
    "# Send the request to register a model group\n",
    "response = client.transport.perform_request(method=\"POST\", url=url)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Check if the task is completed\"\n",
    "\n",
    "# Define the URL for the GET request\n",
    "url = f\"/_plugins/_ml/tasks/UW6T640BWePCYCOKL35W\"#{response['task_id']}\" #Mc3_wo0B8eW0xskyRIm5\"\n",
    "\n",
    "# Send the request to register a model group\n",
    "response = client.transport.perform_request(method=\"GET\", url=url)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create an ingest pipeline for neural search\n",
    "Neural search uses a language model to transform text into vector embeddings.\n",
    " During ingestion, neural search creates vector embeddings for the text fields\n",
    "   in the request. During search, you can generate vector embeddings for the query\n",
    "     text by applying the same model, allowing you to perform vector similarity search\n",
    "       on the documents.\n",
    "       \n",
    "set up a text_embedding processor that creates vector embeddings from text.\n",
    " need the model_id of the model that was set up in the previous section and\n",
    "   a field_map, which specifies the name of the field from which to take the text\n",
    "     (text) and the name of the field in which to record embeddings (vector):\n",
    "       \"\"\"\n",
    "\n",
    "# Define the URL for the PUT request\n",
    "url = \"/_ingest/pipeline/nlp-ingest-pipeline\"\n",
    "\n",
    "# Define the request body\n",
    "request_body = {\n",
    "    \"description\": \"An NLP ingest pipeline\",\n",
    "    \"processors\": [\n",
    "        {\n",
    "            \"text_embedding\": {\n",
    "                \"model_id\": f\"{response['model_id']}\",#\"nfH5wo0BdZ4UfKGGQ7gm\",\n",
    "                \"field_map\": {\n",
    "                    \"text\": \"vector\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send the PUT request to create the NLP ingest pipeline\n",
    "response = client.transport.perform_request(method=\"PUT\", url=url, body=request_body)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a k-NN index\n",
    "create a k-NN index with a field named text, and a knn_vector field named vector,\n",
    " which contains the vector embedding of the text. Additionally, set the default ingest pipeline\n",
    "   to the nlp-ingest-pipeline.\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Types of splitting and embedding-max_length-Dimension\n",
    "-pubmed-embedding-256-384 Models: huggingface/sentence-transformers/all-MiniLM-L6-v2\n",
    "-pubmed-embedding-384-768 Models: huggingface/sentence-transformers/all-mpnet-base-v2\n",
    "-pubmed-embedding-512-768 Models: huggingface/sentence-transformers/all-distilroberta-v1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "    Settings: This section contains configuration settings for the index.\n",
    "\n",
    "    index: This sub-section contains index-level settings.\n",
    "\n",
    "    knn: This setting enables K-Nearest Neighbors (KNN) functionality on the index. When set to true, it allows the index to store and search KNN vectors efficiently.\n",
    "\n",
    "    knn.algo_param.ef_search: This parameter specifies the \"ef_search\" value used by the KNN algorithm. \"ef_search\" controls the quality of the search results and \n",
    "        the speed of the search operation. Higher values generally improve search quality but may increase search time. In this case, it's set to 100.\n",
    "        \"settings\": {\n",
    "                \"index\": {\n",
    "                \"knn\": True,\n",
    "                \"knn.algo_param.ef_search\": 100\n",
    "                }\n",
    "\n",
    "    Properties: This is where you define the fields and their properties within your index. Each property represents a field in your documents.\n",
    "\n",
    "    id: This is a field of type text. It could be used to store a unique identifier for each document.\n",
    "\n",
    "    vector: This is a field of type knn_vector. KNN vectors are used to store dense vectors that are suitable for similarity searches\n",
    "      using K-Nearest Neighbors (KNN) algorithms.\n",
    "\n",
    "    dimension: Specifies the dimensionality of the vectors. In this case, it's set to 768, indicating that each embedding vector has 768 dimensions.\n",
    "\n",
    "    method: Specifies the method used to index and search the vectors.\n",
    "\n",
    "    engine: The underlying search engine. In this case, it's set to \"lucene\", indicating that the Lucene engine is used.\n",
    "\n",
    "    space_type: The space type used for similarity calculation. In this case, it's set to \"l2\", which refers to Euclidean distance.\n",
    "\n",
    "    name: The name of the algorithm used for indexing and searching vectors. Here, it's set to \"hnsw\", which stands for Hierarchical Navigable Small World.\n",
    "\n",
    "    parameters: Additional parameters for the chosen algorithm. These parameters control the construction process of the index structure and affect its performance and accuracy.\n",
    "            \"parameters\": {\n",
    "              \"ef_construction\": 128,\n",
    "              \"m\": 24\n",
    "\n",
    "    text: This is another field of type text. It could be used to store textual data such as the content of documents.\"\"\"\n",
    "# Define the index name\n",
    "#pubmed-distilroberta-recursive-500-50  max=512 dim=768\n",
    "#pubmed-mpnet-recursive-380-50          max=384 dim=768\n",
    "#pubmed-minlmv6-recursive-250-25        max=256 dim=384\n",
    "index_name = 'pubmed-distilroberta-recursive-400-50'\n",
    "\n",
    "\n",
    "# Define the index configuration\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"default_pipeline\": \"nlp-ingest-pipeline\"\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"text\"},\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"method\": {\n",
    "                    \"engine\": \"lucene\",\n",
    "                    \"space_type\": \"cosinesimil\",#\"l2\",\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"parameters\": {}\n",
    "                }\n",
    "            },\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"resource\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check if the index already exists\n",
    "if not client.indices.exists(index=index_name):\n",
    "    # If the index does not exist, create it\n",
    "    response = client.indices.create(index=index_name, body=index_body)\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"Index\", index_name, \"already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Get the settings of the index\"\n",
    "# Define the URL for the GET request\n",
    "url = f\"/{index_name}/_settings\"\n",
    "\n",
    "# Send the GET request to retrieve the index settings\n",
    "response = client.transport.perform_request(method=\"GET\", url=url)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Get the mappins of the index\"\n",
    "# Define the URL for the GET request\n",
    "url = f\"/{index_name}/_mappings\"\n",
    "\n",
    "# Send the GET request to retrieve the index settings\n",
    "response = client.transport.perform_request(method=\"GET\", url=url)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data to the OpenSearch index\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "# Function to remove NaN values recursively\n",
    "def remove_nan(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: remove_nan(v) for k, v in obj.items() if v is not None and not isinstance(v, float) and not (isinstance(v, str) and v.lower() == 'nan')}\n",
    "    elif isinstance(obj, list):\n",
    "        return [remove_nan(elem) for elem in obj if elem is not None and not isinstance(elem, float) and not (isinstance(elem, str) and elem.lower() == 'nan')]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "#data_distilroberta_recursive_500_50\n",
    "#data_mpnet_recursive_380_50\n",
    "#data_minilmv6_recursive_250_25\n",
    "\n",
    "file_name = 'data_distilroberta_recursive_400_50'\n",
    "# # Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv(f'{file_name}.csv')\n",
    "\n",
    "# # Convert the DataFrame to a list of dictionaries (JSON format)\n",
    "# docs = df.to_dict(orient='records')\n",
    "\n",
    "# # Remove NaN values from the data\n",
    "# cleaned_data = remove_nan(docs)\n",
    "\n",
    "# # Save the cleaned data back to a JSON file\n",
    "# with open(f'{file_name}.json', 'w') as f:\n",
    "#     json.dump(cleaned_data, f, indent=4)\n",
    "\n",
    "# Read the JSON file\n",
    "with open(f'{file_name}.json', 'r') as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "# Prepare the actions for bulk indexing\n",
    "actions = []\n",
    "for idx, doc in enumerate(docs):\n",
    "    action = {\n",
    "        'index': {\n",
    "            '_index': index_name,  # Specify the index name\n",
    "            '_id': doc['id']  # Use a unique identifier for each document\n",
    "        }\n",
    "    }\n",
    "    actions.append(action)\n",
    "    actions.append(doc)  # Add the document itself as the next action\n",
    "\n",
    "# Perform the bulk operation\n",
    "batch_size = 10\n",
    "\n",
    "for i in tqdm(range(0, len(actions), batch_size)):\n",
    "    batch_actions = actions[i:i+batch_size]\n",
    "    client.bulk(batch_actions) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "response = client.index(\n",
    "    index = index_name,\n",
    "    body = actions[3],\n",
    "    id = actions[3]['id'],\n",
    "    refresh = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for the GET request\n",
    "url = f\"/{index_name}/_doc/{actions[1]['id']}\"\n",
    "\n",
    "# Send the GET request to retrieve the document\n",
    "response = client.transport.perform_request(method=\"GET\", url=url)\n",
    "\n",
    "# Print the response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Search using a keyword search\"\n",
    "\n",
    "# Define the URL for the GET request\n",
    "url = f\"/{index_name}/_search\"\n",
    "q = \"who is Moog?\"\n",
    "# Define the request body\n",
    "request_body = {\n",
    "    \"_source\": {\n",
    "        \"excludes\": [\"vector\"]\n",
    "    },\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"text\": {\n",
    "                \"query\": q\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the GET request to search for documents\n",
    "response = client.transport.perform_request(method=\"GET\", url=url, body=request_body)\n",
    "\n",
    "# Print the response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Search using a keyword search\"\n",
    "# Define the search query\n",
    "query = {\n",
    "    \"size\": 3,  # Number of hits to return\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"text\": {\n",
    "                \"query\": q\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "respose = client.search(body = query, index= index_name)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Search using a neural search\"\n",
    "# Define the URL for the GET request\n",
    "url = f\"/{index_name}/_search\"\n",
    "\n",
    "# Define the request body\n",
    "request_body = {\n",
    "    \"_source\": {\n",
    "        \"excludes\": [\"vevtor\"]\n",
    "    },\n",
    "    \"size\": 3,\n",
    "    \"query\": {\n",
    "        \"neural\": {\n",
    "            \"vector\": {\n",
    "                \"query_text\": q,\n",
    "                \"model_id\": \"ZnT6zI0B5EQu2yufwsQj\",\n",
    "                \"k\": 3\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the GET request to search for similar documents\n",
    "response = client.transport.perform_request(method=\"GET\", url=url, body=request_body)\n",
    "\n",
    "# Print the response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Configure a search pipeline\"\n",
    "\n",
    "# Define the URL for the PUT request\n",
    "url = \"/_search/pipeline/nlp-search-pipeline\"\n",
    "\n",
    "# Define the request body\n",
    "request_body = {\n",
    "  \"description\": \"Post processor for hybrid search\",\n",
    "  \"phase_results_processors\": [\n",
    "    {\n",
    "      \"normalization-processor\": {\n",
    "        \"normalization\": {\n",
    "          \"technique\": \"min_max\"\n",
    "        },\n",
    "        \"combination\": {\n",
    "          \"technique\": \"arithmetic_mean\",\n",
    "          \"parameters\": {\n",
    "            \"weights\": [\n",
    "              0.3,\n",
    "              0.7\n",
    "            ]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Send the PUT request to create the pipeline\n",
    "response = client.transport.perform_request(method=\"PUT\", url=url, body=request_body)\n",
    "\n",
    "# Print the response\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"When the model is registered, it is saved in the model index.\n",
    " Next, deploy the model. Deploying a model creates a model instance \n",
    " and caches the model in memory.\"\"\"\n",
    "\n",
    "# Define the URL for the GET request {response['model_id']}\n",
    "url = f\"/_plugins/_ml/models/ZnT6zI0B5EQu2yufwsQj/_deploy\" #nfH5wo0BdZ4UfKGGQ7gm\n",
    "\n",
    "# Send the request to register a model group\n",
    "response = client.transport.perform_request(method=\"POST\", url=url)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "index_name = 'pubmed-distilroberta-recursive-400-50'\n",
    "q = \"who is Moog?\"\n",
    "# Define the URL for the GET request including the search pipeline\n",
    "url = f\"/{index_name}/_search?search_pipeline=nlp-search-pipeline\"\n",
    "\n",
    "# Define the request body\n",
    "request_body = {\n",
    "    \"_source\": {\n",
    "        \"exclude\": [\"vector\"]\n",
    "    },\n",
    "    \"size\": 3,\n",
    "    \"query\": {\n",
    "        \"hybrid\": {\n",
    "            \"queries\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"text\": {\n",
    "                            \"query\": q\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"neural\": {\n",
    "                        \"vector\": {\n",
    "                            \"query_text\": q,\n",
    "                            \"model_id\": \"ZnT6zI0B5EQu2yufwsQj\",\n",
    "                            \"k\": 8\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the GET request with the search pipeline\n",
    "response = client.transport.perform_request(method=\"GET\", url=url, body=request_body)\n",
    "\n",
    "# Print the response\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['hits']['hits'][0]['_source']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import api\n",
    "connector = api.gpt.OpenAIConnector()\n",
    "\n",
    "context = []\n",
    "for i, doc in enumerate(response['hits']['hits']):\n",
    "    context.append({\n",
    "        'id': doc['_id'],\n",
    "        'text': doc['_source']['text'],\n",
    "        'source': doc['_source']['resource']\n",
    "    })\n",
    "system_prompt1 = \"You are a researcher on Medical Intelligence, that can answer questions based on the provided articles.\"\n",
    "system_prompt2 = \"You are a friendly Assistant that will answer Questions based on given Contexts.\"\n",
    "user_prompt1 = f\"1- Answer the question with the regarding all chunked Contexts.2- If there are more than one answer provide all of them with resources.\\n 3- If it is not possible to answer based on given contexts, Explicitly say that and answer based on your knowledge.\\n4- provide the resources for each chunk at the end of your message.\\n 5- If there is previous answers from you use them as well with reference \\nQuestion:\\n{q} \\nContexts:\\n{context}\"\n",
    "user_prompt2 = f\"Answer the following question: {q} based on the following chunked texts: {context}. 1- If there is no answer based on provided texts, just say 'I cannot provide an answer based on the provided text.'\"\n",
    "message = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt1},  \n",
    "    {\"role\": \"user\", \"content\": user_prompt1},\n",
    "]\n",
    "correction = connector.get_completions(message)\n",
    "correction.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.delete(\n",
    "#     index = 'python-test-index',\n",
    "#     id = '1'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.indices.delete(\n",
    "#     index = 'pubmed-articles'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Delete the ingest pipeline\"\n",
    "# # Define the URL for the DELETE request\n",
    "# url = \"/_ingest/pipeline/nlp-ingest-pipeline\"\n",
    "\n",
    "# # Send the DELETE request to remove the NLP ingest pipeline\n",
    "# response = client.transport.perform_request(method=\"DELETE\", url=url)\n",
    "\n",
    "# response"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
