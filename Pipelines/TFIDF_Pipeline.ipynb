{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conbine different columns of the dataset into one column\n",
    "import pandas as pd\n",
    "\n",
    "# Read the original CSV file\n",
    "df_part = pd.read_csv('articles.csv', index_col='PMID', usecols=['PMID', 'TI', 'AB', 'FAU', 'DP', 'OT', 'JT', 'MH'])\n",
    "\n",
    "# Create a new DataFrame with the desired structure\n",
    "new_df = pd.DataFrame(index=df_part.index)\n",
    "\n",
    "# Combine the information into a single column\n",
    "new_df['CD'] = (\n",
    "    'PMID: ' + df_part.index.astype(str) + '\\n' +\n",
    "    'Abstract: ' + df_part['AB'].fillna('None') + '\\n' +\n",
    "    'Title: ' + df_part['TI'].fillna('None') + '\\n' +\n",
    "    'Authors: ' + df_part['FAU'].fillna('None') + ',\\n' +\n",
    "    'Data of Publication: ' + df_part['DP'].fillna('None') + '\\n' +\n",
    "    'Terms or keywords associated with the article: ' + df_part['OT'].fillna('None') + '\\n' +\n",
    "    'Journal Title: ' + df_part['JT'].fillna('None') + '\\n' +\n",
    "    'Medical subject headings: ' + df_part['MH'].fillna('None') + '\\n'# +\n",
    "    # 'Abstract: ' + df_part['AB'].fillna('None')\n",
    ")\n",
    "new_df['source'] = 'https://pubmed.ncbi.nlm.nih.gov/' + df_part.index.astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# Function to filter out lines ending with 'None' from a given text\n",
    "def filter_lines(text):\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = [line for line in lines if not line.strip().endswith('None')]\n",
    "    return ', '.join(filtered_lines)\n",
    "\n",
    "# Apply the filtering function to each row in the 'CD' column\n",
    "new_df['CD'] = new_df['CD'].apply(filter_lines)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "new_df.to_csv('additional_data.csv')\n",
    "# Print the DataFrame with the filtered 'CD' column\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import LlamaTokenizer, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "docs = pd.read_csv('additional_data.csv')\n",
    "# docs['Combined_Info'] = docs['Combined_Info'].str.replace('|', ' ')\n",
    "\n",
    "hf_auth = os.environ.get('HF_AUTH')\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\",use_auth_token=hf_auth)\n",
    "# # length of tokenized input\n",
    "# def token_len(text):\n",
    "#     tokens = tokenizer.encode(text)\n",
    "#     return len(tokens)\n",
    "\n",
    "#sentence-transformers/all-distilroberta-v1\n",
    "#sentence-transformers/all-mpnet-base-v2\n",
    "#sentence-transformers/all-MiniLM-L6-v2\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-distilroberta-v1')\n",
    "# length of tokenized input\n",
    "def token_len(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "# stats for tokenized input But not necessary\n",
    "token_counts = [token_len(docs['CD'][i]) for i, _ in enumerate(docs['CD'])]\n",
    "min_tokens=min(token_counts)\n",
    "avg_tokens=int(sum(token_counts) / len(token_counts))\n",
    "max_tokens=max(token_counts)\n",
    "print(f\"\"\"Min: {min_tokens}\n",
    "Avg: {avg_tokens}\n",
    "Max: {max_tokens}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens(tokenizer(\"PMID 24278995 Title CASK Disorders Authors Moog Ute Kutsche Kerstin Data of Publication 1993 Terms or keywords associated with the article Intellectual Disability and Microcephaly with Pontine and Cerebellar Hypoplasia MICPCH XLinked Intellectual Disability XLID with or without Nystagmus Peripheral plasma membrane\").input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority is by the length of chunk and overlap,\n",
    "# if they don't exceed the default values, the separator will be used\n",
    "from langchain.text_splitter import NLTKTextSplitter,CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,\n",
    "    length_function=token_len,\n",
    "    # separators=['\\n\\n', '\\n', ' ', '']\n",
    ")\n",
    "\n",
    "# test the text splitter\n",
    "chunks = text_splitter.split_text(docs['CD'][0])\n",
    "print(f\"length of chunk: {len(chunks)}\")\n",
    "print(f\"Content of chunk0:\\n{chunks[0]}\")\n",
    "print('the length of chunk 0 is:', len(chunks[0]))\n",
    "print(\"*\"*100)\n",
    "print(f\"Content of chunk1:\\n{chunks[1]}\")\n",
    "print('the length of chunk 1 is:', len(chunks[1]))\n",
    "print(\"*\"*100)\n",
    "print(f\"Content of chunk1:\\n{chunks[2]}\")\n",
    "print('the length of chunk 2 is:', len(chunks[2]))\n",
    "print(\"*\"*100)\n",
    "# print(f\"Content of chunk1:\\n{chunks[3]}\")\n",
    "# print('the length of chunk 3 is:', len(chunks[3]))\n",
    "# print(\"*\"*100)\n",
    "# print(f\"Content of chunk1:\\n{chunks[4]}\")\n",
    "# print('the length of chunk 4 is:', len(chunks[4]))\n",
    "# print(\"*\"*100)\n",
    "# print(f\"Content of chunk1:\\n{chunks[5]}\")\n",
    "# print('the length of chunk 5 is:', len(chunks[5]))\n",
    "# print(\"*\"*100)\n",
    "# print(f\"Content of chunk1:\\n{chunks[6]}\")\n",
    "# print('the length of chunk 6 is:', len(chunks[6]))\n",
    "# print(\"*\"*100)\n",
    "# print(f\"Content of chunk1:\\n{chunks[7]}\")\n",
    "# print('the length of last chunk is:', len(chunks[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# import re\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "#     # Define the pattern to match punctuation\n",
    "#     punctuation_pattern = r'[^\\w\\s]'\n",
    "    \n",
    "#     # Use regex to substitute punctuation with an empty string\n",
    "#     text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    \n",
    "#     return text_without_punctuation\n",
    "\n",
    "documents=[]\n",
    "for j, doc in tqdm(enumerate(docs['CD'])):\n",
    "    # chunks = text_splitter.split_text(remove_punctuation(doc))\n",
    "    chunks = text_splitter.split_text(doc)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append({\n",
    "            'id': f\"{docs['PMID'][j]}-{i}\",\n",
    "            'text': chunk,\n",
    "            'resource': docs['source'][j],\n",
    "        })\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a DataFrame\n",
    "import pandas as pd\n",
    "# data = pd.DataFrame(documents)\n",
    "# data.to_csv('data_distilroberta_recursive_400_50.csv', index=False)\n",
    "data = pd.read_csv('data_distilroberta_recursive_400_50.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # Load the model\n",
    "# model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "\n",
    "# # Input sentence\n",
    "# # sentence = \"variant has been identifiedin an affected family member, prenatal testing for a pregnancy at increased riskand preimplantation genetic testing for a CASK disorder are possible., Title: CASK Disorders., Authors: Moog, Ute|Kutsche, Kerstin,, Data of Publication: 1993, Terms or keywords associated with the article: Intellectual Disability and Microcephaly with Pontine and Cerebellar Hypoplasia (MICPCH)|X-Linked Intellectual Disability (XLID) with or without Nystagmus|Peripheral plasma membrane protein CASK|CASK|CASK Disorders,\"\n",
    "\n",
    "# # Get the sentence embedding\n",
    "# embedding = model.encode(sentence)\n",
    "\n",
    "# # Output the embedded representation\n",
    "# print(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# # Tokenize the sentence\n",
    "# tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# # Output the tokens\n",
    "# print(tokens)\n",
    "# print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the the top 5 relevent articles for a given query using TF-IDF and cosine similarity\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load CSV data\n",
    "# additional_docs = pd.read_csv('additional_data.csv')\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "# Using charactor based vectorizer cannot find the names of the authors\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['text'].fillna(''))\n",
    "\n",
    "# Function to search for queries\n",
    "def search(query, tfidf_matrix, data):\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = linear_kernel(query_vector, tfidf_matrix).flatten()\n",
    "    document_scores = list(enumerate(cosine_similarities))\n",
    "    document_scores = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top N results\n",
    "    top_results = document_scores[:5]\n",
    "    for idx, score in top_results:\n",
    "        if score != 0:\n",
    "            print(f\"ID: {data['id'].iloc[idx]}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "search(\"who is Moog?\", tfidf_matrix, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 implementation, that is not recommendeed \n",
    "# as it produce the same result but takes much longer time \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    def __init__(self, b=0.75, k1=1.6):\n",
    "        self.vectorizer = TfidfVectorizer(norm=None, smooth_idf=False)\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Fit IDF to documents X \"\"\"\n",
    "        self.vectorizer.fit(X)\n",
    "        y = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        self.avdl = y.sum(1).mean()\n",
    "\n",
    "    def transform(self, q, X):\n",
    "        \"\"\" Calculate BM25 between query q and documents X \"\"\"\n",
    "        b, k1, avdl = self.b, self.k1, self.avdl\n",
    "\n",
    "        # apply CountVectorizer\n",
    "        X = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        len_X = X.sum(1).A1\n",
    "        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
    "        assert sparse.isspmatrix_csr(q)\n",
    "\n",
    "        # convert to csc for better column slicing\n",
    "        X = X.tocsc()[:, q.indices]\n",
    "        denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n",
    "        # idf(t) = log [ n / df(t) ] + 1 in sklearn, so it need to be coneverted\n",
    "        # to idf(t) = log [ n / df(t) ] with minus 1\n",
    "        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
    "        numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)                                                          \n",
    "        return (numer / denom).sum(1).A1\n",
    "\n",
    "# Example usage\n",
    "\n",
    "query = \"who is Moog?\"\n",
    "# search(query, bm25_matrix, data)\n",
    "def search(query, data):\n",
    "    bm25 = BM25()\n",
    "    bm25.fit(data['text'])\n",
    "    scores = bm25.transform(query, data['text'])\n",
    "    document_scores = list(enumerate(scores))\n",
    "    document_scores = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top N results\n",
    "    top_results = document_scores[:5]\n",
    "    for idx, score in top_results:\n",
    "        if score != 0:\n",
    "            print(f\"ID: {data['id'].iloc[idx]}, Score: {score}\")\n",
    "\n",
    "search(query, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Load CSV data\n",
    "# additional_docs = pd.read_csv('additional_data.csv')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_texts = [text.split() for text in data['text'].fillna('')]\n",
    "\n",
    "# Create a BM25 model\n",
    "bm25 = BM25Okapi(tokenized_texts)\n",
    "\n",
    "# Function to search for queries\n",
    "def search(query, bm25, tokenized_texts, data):\n",
    "    scores = bm25.get_scores(query.split())\n",
    "    top_results = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for idx, score in top_results:\n",
    "        if score != 0:\n",
    "            print(f\"ID: {data['id'].iloc[idx]}, Score: {score}\")\n",
    "\n",
    "# Example usage\n",
    "query = \"who is Moog?\"\n",
    "search(query, bm25, tokenized_texts, data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
